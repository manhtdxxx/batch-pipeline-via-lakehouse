services:
  airflow:
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    image: my-airflow
    container_name: airflow
    restart: unless-stopped
    ports:
      - 8081:8080
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW_CONN_SPARK_CONN=spark://spark-master:7077 # if using SparkSubmitOperator
      - AIRFLOW_CONN_SSH_SPARK=ssh://spark_user:spark_pass@spark-master:22 # if using SSHOperator
    command: >
      bash -c "
      airflow db migrate &&
      airflow users create --username airflow --password airflow --firstname manh --lastname td --role Admin --email tdmanh1510@gmail.com;
      airflow webserver &
      exec airflow scheduler
      "
    volumes:
      - ./src/dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
    networks:
      - common-net

  # superset:
  #   build:
  #     context: ./docker/superset
  #     dockerfile: Dockerfile
  #   image: my-superset
  #   container_name: superset
  #   restart: unless-stopped
  #   command: >
  #     /bin/bash -c "
  #       superset db upgrade &&
  #       superset fab create-admin --username superset --password superset --firstname Superset --lastname Admin --email tdmanh1510@gmail.com &&
  #       superset init &&
  #       superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger
  #     "
  #   ports:
  #     - 8088:8088
  #   volumes:
  #     - superset-data:/app/superset_home
  #   networks:
  #     - common-net

volumes:
  airflow-db:
    name: airflow-db
  # superset-data:
  #   name: superset-data

networks:
  common-net:
    external: true
